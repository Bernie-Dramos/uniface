{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üëÅÔ∏è Gaze Estimation with UniFace\n",
        "\n",
        "<div style=\"display:flex; flex-wrap:wrap; align-items:center;\">\n",
        "  <a style=\"margin-right:10px; margin-bottom:6px;\" href=\"https://pepy.tech/projects/uniface\"><img alt=\"PyPI Downloads\" src=\"https://static.pepy.tech/badge/uniface\"></a>\n",
        "  <a style=\"margin-right:10px; margin-bottom:6px;\" href=\"https://pypi.org/project/uniface/\"><img alt=\"PyPI Version\" src=\"https://img.shields.io/pypi/v/uniface.svg\"></a>\n",
        "  <a style=\"margin-right:10px; margin-bottom:6px;\" href=\"https://opensource.org/licenses/MIT\"><img alt=\"License\" src=\"https://img.shields.io/badge/License-MIT-blue.svg\"></a>\n",
        "  <a style=\"margin-bottom:6px;\" href=\"https://github.com/yakhyo/uniface\"><img alt=\"GitHub Stars\" src=\"https://img.shields.io/github/stars/yakhyo/uniface.svg?style=social\"></a>\n",
        "</div>\n",
        "\n",
        "**UniFace** is a lightweight, production-ready, all-in-one face analysis library built on ONNX Runtime.\n",
        "\n",
        "üîó **GitHub**: [github.com/yakhyo/uniface](https://github.com/yakhyo/uniface) | üìö **Docs**: [yakhyo.github.io/uniface](https://yakhyo.github.io/uniface)\n",
        "\n",
        "---\n",
        "\n",
        "## üìñ Overview\n",
        "\n",
        "This notebook demonstrates **gaze estimation** - predicting where a person is looking:\n",
        "\n",
        "- ‚úÖ Estimate gaze direction (pitch and yaw angles)\n",
        "- ‚úÖ Visualize gaze with direction arrows\n",
        "- ‚úÖ Process multiple face images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1Ô∏è‚É£ Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -q uniface\n",
        "\n",
        "import os\n",
        "import urllib.request\n",
        "\n",
        "os.makedirs('assets/test_images', exist_ok=True)\n",
        "\n",
        "BASE_URL = \"https://raw.githubusercontent.com/yakhyo/uniface/main/assets\"\n",
        "images = [\"test_images/image0.jpg\", \"test_images/image1.jpg\", \"test_images/image2.jpg\",\n",
        "          \"test_images/image3.jpg\", \"test_images/image4.jpg\"]\n",
        "\n",
        "for img in images:\n",
        "    if not os.path.exists(f'assets/{img}'):\n",
        "        urllib.request.urlretrieve(f\"{BASE_URL}/{img}\", f\"assets/{img}\")\n",
        "        print(f\"‚úì Downloaded {img}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2Ô∏è‚É£ Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "\n",
        "import uniface\n",
        "from uniface.detection import RetinaFace\n",
        "from uniface.gaze import MobileGaze\n",
        "from uniface.visualization import draw_gaze\n",
        "\n",
        "print(f\"UniFace version: {uniface.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3Ô∏è‚É£ Initialize Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "detector = RetinaFace(confidence_threshold=0.5)\n",
        "gaze_estimator = MobileGaze()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4Ô∏è‚É£ Process All Test Images\n",
        "\n",
        "We'll detect faces, estimate gaze, and visualize results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_images_dir = Path('assets/test_images')\n",
        "test_images = sorted(test_images_dir.glob('*.jpg'))\n",
        "\n",
        "original_images = []\n",
        "processed_images = []\n",
        "\n",
        "for image_path in test_images:\n",
        "    print(f\"Processing: {image_path.name}\")\n",
        "\n",
        "    image = cv2.imread(str(image_path))\n",
        "    original = image.copy()\n",
        "\n",
        "    faces = detector.detect(image)\n",
        "    print(f'   ‚úì Detected {len(faces)} face(s)')\n",
        "\n",
        "    for i, face in enumerate(faces):\n",
        "        x1, y1, x2, y2 = map(int, face.bbox[:4])\n",
        "        face_crop = image[y1:y2, x1:x2]\n",
        "\n",
        "        if face_crop.size > 0:\n",
        "            gaze = gaze_estimator.estimate(face_crop)\n",
        "            pitch_deg = np.degrees(gaze.pitch)\n",
        "            yaw_deg = np.degrees(gaze.yaw)\n",
        "\n",
        "            print(f'   ‚Üí Face {i+1}: pitch={pitch_deg:.1f}¬∞, yaw={yaw_deg:.1f}¬∞')\n",
        "            draw_gaze(image, face.bbox, gaze.pitch, gaze.yaw, draw_angles=False)\n",
        "\n",
        "    original_rgb = cv2.cvtColor(original, cv2.COLOR_BGR2RGB)\n",
        "    processed_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    original_images.append(original_rgb)\n",
        "    processed_images.append(processed_rgb)\n",
        "\n",
        "print(f\"\\n‚úì Processed {len(test_images)} images\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5Ô∏è‚É£ Visualize Results\n",
        "\n",
        "**Row 1**: Original images  \n",
        "**Row 2**: Images with gaze direction arrows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_images = len(original_images)\n",
        "\n",
        "fig, axes = plt.subplots(2, num_images, figsize=(4*num_images, 8))\n",
        "\n",
        "if num_images == 1:\n",
        "    axes = axes.reshape(2, 1)\n",
        "\n",
        "for i, img in enumerate(original_images):\n",
        "    axes[0, i].imshow(img)\n",
        "    axes[0, i].set_title(f'Original {i+1}', fontsize=12)\n",
        "    axes[0, i].axis('off')\n",
        "\n",
        "for i, img in enumerate(processed_images):\n",
        "    axes[1, i].imshow(img)\n",
        "    axes[1, i].set_title(f'Gaze {i+1}', fontsize=12)\n",
        "    axes[1, i].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìù Summary\n",
        "\n",
        "| Feature | Description |\n",
        "|---------|-------------|\n",
        "| **Input** | Face crop from face detection |\n",
        "| **Output** | `GazeResult` with `pitch` and `yaw` (radians) |\n",
        "| **Pitch** | Up/down angle (+ = looking up) |\n",
        "| **Yaw** | Left/right angle (+ = looking right) |\n",
        "| **Model** | MobileGaze (trained on Gaze360 dataset) |\n",
        "| **Accuracy** | MAE ~11-13 degrees |\n",
        "\n",
        "### Tips for Best Results\n",
        "\n",
        "- ‚úÖ Ensure faces are clearly visible and well-lit\n",
        "- ‚úÖ Works best with frontal to semi-profile faces\n",
        "- ‚ö†Ô∏è Accuracy may vary with extreme head poses\n",
        "- ‚ö†Ô∏è Occlusions (sunglasses, hair) may affect results\n",
        "\n",
        "---\n",
        "\n",
        "## üîó Explore More Notebooks\n",
        "\n",
        "| Notebook | Description |\n",
        "|----------|-------------|\n",
        "| [01_face_detection](./01_face_detection.ipynb) | Detect faces with bounding boxes |\n",
        "| [02_face_alignment](./02_face_alignment.ipynb) | Align faces for recognition |\n",
        "| [03_face_verification](./03_face_verification.ipynb) | Compare two faces |\n",
        "| [04_face_search](./04_face_search.ipynb) | Find a person in a crowd |\n",
        "| [05_face_analyzer](./05_face_analyzer.ipynb) | Age & gender prediction |\n",
        "| [06_face_parsing](./06_face_parsing.ipynb) | Semantic segmentation |\n",
        "| [07_face_anonymization](./07_face_anonymization.ipynb) | Privacy protection |\n",
        "\n",
        "üìö **Full Documentation**: [yakhyo.github.io/uniface](https://yakhyo.github.io/uniface)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
