{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üé≠ Face Parsing: Semantic Segmentation\n",
        "\n",
        "<div style=\"display:flex; flex-wrap:wrap; align-items:center;\">\n",
        "  <a style=\"margin-right:10px; margin-bottom:6px;\" href=\"https://pepy.tech/projects/uniface\"><img alt=\"PyPI Downloads\" src=\"https://static.pepy.tech/badge/uniface\"></a>\n",
        "  <a style=\"margin-right:10px; margin-bottom:6px;\" href=\"https://pypi.org/project/uniface/\"><img alt=\"PyPI Version\" src=\"https://img.shields.io/pypi/v/uniface.svg\"></a>\n",
        "  <a style=\"margin-right:10px; margin-bottom:6px;\" href=\"https://opensource.org/licenses/MIT\"><img alt=\"License\" src=\"https://img.shields.io/badge/License-MIT-blue.svg\"></a>\n",
        "  <a style=\"margin-bottom:6px;\" href=\"https://github.com/yakhyo/uniface\"><img alt=\"GitHub Stars\" src=\"https://img.shields.io/github/stars/yakhyo/uniface.svg?style=social\"></a>\n",
        "</div>\n",
        "\n",
        "**UniFace** is a lightweight, production-ready, all-in-one face analysis library built on ONNX Runtime.\n",
        "\n",
        "üîó **GitHub**: [github.com/yakhyo/uniface](https://github.com/yakhyo/uniface) | üìö **Docs**: [yakhyo.github.io/uniface](https://yakhyo.github.io/uniface)\n",
        "\n",
        "---\n",
        "\n",
        "## üìñ Overview\n",
        "\n",
        "This notebook demonstrates **face parsing** - semantic segmentation of facial components:\n",
        "\n",
        "- ‚úÖ Segment faces into 19 different components (skin, eyes, hair, etc.)\n",
        "- ‚úÖ Visualize parsing results with color overlays\n",
        "- ‚úÖ Extract specific facial components using masks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1Ô∏è‚É£ Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -q uniface\n",
        "\n",
        "import os\n",
        "import urllib.request\n",
        "\n",
        "os.makedirs('assets/test_images', exist_ok=True)\n",
        "\n",
        "BASE_URL = \"https://raw.githubusercontent.com/yakhyo/uniface/main/assets\"\n",
        "images = [\"test_images/image0.jpg\", \"test_images/image1.jpg\", \"test_images/image2.jpg\",\n",
        "          \"test_images/image3.jpg\", \"test_images/image4.jpg\"]\n",
        "\n",
        "for img in images:\n",
        "    if not os.path.exists(f'assets/{img}'):\n",
        "        urllib.request.urlretrieve(f\"{BASE_URL}/{img}\", f\"assets/{img}\")\n",
        "        print(f\"‚úì Downloaded {img}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2Ô∏è‚É£ Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "\n",
        "import uniface\n",
        "from uniface.parsing import BiSeNet\n",
        "from uniface.constants import ParsingWeights\n",
        "from uniface.visualization import vis_parsing_maps\n",
        "\n",
        "print(f\"UniFace version: {uniface.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3Ô∏è‚É£ Initialize BiSeNet Parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "parser = BiSeNet(model_name=ParsingWeights.RESNET34)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4Ô∏è‚É£ Parsing Classes Reference\n",
        "\n",
        "The BiSeNet model segments faces into **19 different classes**:\n",
        "\n",
        "| ID | Component | ID | Component |\n",
        "|:--:|-----------|:--:|-----------|\n",
        "| 0 | Background | 10 | Nose |\n",
        "| 1 | Skin | 11 | Mouth |\n",
        "| 2 | Left Eyebrow | 12 | Upper Lip |\n",
        "| 3 | Right Eyebrow | 13 | Lower Lip |\n",
        "| 4 | Left Eye | 14 | Neck |\n",
        "| 5 | Right Eye | 15 | Neck Lace |\n",
        "| 6 | Eye Glasses | 16 | Cloth |\n",
        "| 7 | Left Ear | 17 | Hair |\n",
        "| 8 | Right Ear | 18 | Hat |\n",
        "| 9 | Ear Ring | | |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5Ô∏è‚É£ Process Multiple Face Images\n",
        "\n",
        "The test images are already cropped faces, so we can directly parse them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_images_dir = Path('assets/test_images')\n",
        "test_images = sorted(test_images_dir.glob('*.jpg'))\n",
        "\n",
        "original_images = []\n",
        "parsed_images = []\n",
        "\n",
        "for image_path in test_images:\n",
        "    print(f\"Processing: {image_path.name}\")\n",
        "    image = cv2.imread(str(image_path))\n",
        "    mask = parser.parse(image)\n",
        "    unique_classes = len(set(mask.flatten()))\n",
        "    print(f'   ‚úì Found {unique_classes} unique classes')\n",
        "\n",
        "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    vis_result = vis_parsing_maps(image_rgb, mask, save_image=False)\n",
        "\n",
        "    original_images.append(image_rgb)\n",
        "    parsed_images.append(vis_result)\n",
        "\n",
        "print(f\"\\n‚úì Processed {len(test_images)} images\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6Ô∏è‚É£ Visualize Results\n",
        "\n",
        "**Row 1**: Original face images  \n",
        "**Row 2**: Parsed images with color overlay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_images = len(original_images)\n",
        "fig, axes = plt.subplots(2, num_images, figsize=(4 * num_images, 8))\n",
        "\n",
        "if num_images == 1:\n",
        "    axes = axes.reshape(-1, 1)\n",
        "\n",
        "for i in range(num_images):\n",
        "    axes[0, i].imshow(original_images[i])\n",
        "    axes[0, i].set_title(f'Original {i+1}', fontsize=12)\n",
        "    axes[0, i].axis('off')\n",
        "\n",
        "    axes[1, i].imshow(parsed_images[i])\n",
        "    axes[1, i].set_title(f'Parsed {i+1}', fontsize=12)\n",
        "    axes[1, i].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7Ô∏è‚É£ Detailed Parsing View\n",
        "\n",
        "Let's parse a single face and show the segmentation mask in detail."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "image_path = 'assets/test_images/image1.jpg'\n",
        "image = cv2.imread(image_path)\n",
        "mask = parser.parse(image)\n",
        "\n",
        "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "vis_result = vis_parsing_maps(image_rgb, mask, save_image=False)\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "axes[0].imshow(image_rgb)\n",
        "axes[0].set_title('Original Face', fontsize=14)\n",
        "axes[0].axis('off')\n",
        "\n",
        "axes[1].imshow(mask, cmap='tab20')\n",
        "axes[1].set_title('Segmentation Mask', fontsize=14)\n",
        "axes[1].axis('off')\n",
        "\n",
        "axes[2].imshow(vis_result)\n",
        "axes[2].set_title('Overlay Visualization', fontsize=14)\n",
        "axes[2].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"üìä Mask shape: {mask.shape}\")\n",
        "print(f\"üìä Unique classes: {np.unique(mask)}\")\n",
        "print(f\"üìä Number of classes found: {len(np.unique(mask))}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8Ô∏è‚É£ Extract Specific Facial Components\n",
        "\n",
        "Use the mask to extract individual facial components."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "image_path = 'assets/test_images/image0.jpg'\n",
        "image = cv2.imread(image_path)\n",
        "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "mask = parser.parse(image)\n",
        "\n",
        "components_to_extract = {'Skin': 1, 'Hair': 17, 'Nose': 10, 'Lips': [12, 13]}\n",
        "\n",
        "fig, axes = plt.subplots(1, len(components_to_extract) + 1, figsize=(20, 4))\n",
        "\n",
        "axes[0].imshow(image_rgb)\n",
        "axes[0].set_title('Original', fontsize=12)\n",
        "axes[0].axis('off')\n",
        "\n",
        "for idx, (name, class_ids) in enumerate(components_to_extract.items(), 1):\n",
        "    if isinstance(class_ids, list):\n",
        "        component_mask = np.zeros_like(mask, dtype=np.uint8)\n",
        "        for class_id in class_ids:\n",
        "            component_mask |= (mask == class_id).astype(np.uint8)\n",
        "    else:\n",
        "        component_mask = (mask == class_ids).astype(np.uint8)\n",
        "\n",
        "    extracted = image_rgb.copy()\n",
        "    extracted[component_mask == 0] = 0\n",
        "\n",
        "    axes[idx].imshow(extracted)\n",
        "    axes[idx].set_title(name, fontsize=12)\n",
        "    axes[idx].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìù Summary\n",
        "\n",
        "| Feature | Description |\n",
        "|---------|-------------|\n",
        "| **Model** | BiSeNet (ResNet18 or ResNet34 backbone) |\n",
        "| **Classes** | 19 semantic classes |\n",
        "| **Input** | Face image (cropped or full) |\n",
        "| **Output** | Segmentation mask (H√óW) with class IDs |\n",
        "\n",
        "### Use Cases\n",
        "\n",
        "- üé® Virtual makeup application\n",
        "- üíá Hair color change / style preview\n",
        "- üî¨ Skin analysis and retouching\n",
        "- üé≠ Face editing and composition\n",
        "\n",
        "---\n",
        "\n",
        "## üîó Next Steps\n",
        "\n",
        "- **Face Anonymization**: Blur faces for privacy ‚Üí [07_face_anonymization.ipynb](./07_face_anonymization.ipynb)\n",
        "- **Gaze Estimation**: Predict eye gaze direction ‚Üí [08_gaze_estimation.ipynb](./08_gaze_estimation.ipynb)\n",
        "- **Full Documentation**: [yakhyo.github.io/uniface](https://yakhyo.github.io/uniface)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
